# 容器资源控制
> 内存限额
- **-m 或 --memory** 设置内存的使用限额，例如 100M, 2G
- **--memory-swap** 设置 内存+swap 的使用限额
- 只指定 -m 而不指定 --memory-swap，那么 --memory-swap 默认为 -m 的两倍

> CPU 权重
- **-c 或 --cpu-shares** 默认1024

> Block IO 权重   
- **--blkio-weight** 目前 Block IO 限额只对 direct IO（不使用文件缓存）有效
- bps 是 byte per second，每秒读写的数据量
- iops 是 io per second，每秒 IO 的次数
- **--device-read-bps** 限制读某个设备的bps
- **--device-write-bps** 限制写某个设备的bps
- **--device-read-iops** 限制读某个设备的iops
- **--device-write-iops** 限制写某个设备的iops
- 例: docker run -it --device-write-bps /dev/sda:30MB ubuntu

# 容器底层实现
> cgroup 
- 全称 Control Group 设置进程使用 CPU、内存 和 IO 资源的限额
- 作用目录 /sys/fs/cgroup/*/docker/容器ID

> namespace 
- 实现容器间资源隔离，六种：Mount、UTS、IPC、PID、Network 和 User
- Mount   让容器拥有自己的文件系统
- UTS     让容器拥有自己的hostname，默认名是短ID，可通过 -h 或 --hostname 参数设置
- IPC     让容器拥有自己的共享内存和信号量（semaphore）来实现进程间通信
- PID     让容器拥有自己的PID进程
- Network 让容器拥有自己的网卡、IP、路由等资源
- User    让容器拥有自己的用户，host不能看到容器中创建的用户

# 容器网络和通信
> 单host容器网络
- 安装时会创建三个网络，**docker network ls** 查看：none、host、bridge
- none 什么都没有的网络，挂在它下容器除了lo，没有其他任何网卡。容器创建时，可通过 --network=none 使用none网络，一些对安全性要求高并且无需联网的应用可使用none网络
- host 容器共享Docker host网络栈。可通过 --network=host 使用host网络，性能最好，对网络传输效率有较高要求，可选host网络
- bridge 容器默认的网络。使用 **brctl show** 查看bridge网络，veth pair 是一种成对出现的特殊网络设备，可看成由一根虚拟网线连接一对网卡，一头在容器中（**ip a** 可看到，如 eth0），另一头（如：veth080bba8）挂在网桥docker0上
- **docker network inspect bridge** 查看bridge网络配置信息

> 用户自定义网络
- user-defined 三种网络驱动：bridge, overlay 和 macvlan。
- **docker network create --driver bridge my_net** 创建自定义网络，**--subnet** 和 **--gateway** 参数指定IP段和网关。
- 容器要使用自定义网络，需在启动时通过 --network 指定，通过 --ip 指定静态IP（只有使用 --subnet 创建的网络才能指定静态IP）
- **docker network connect my_net 容器ID** 把容器加入到指定网络

> 容器之间通信
- 可通过 IP，Docker DNS Server 或 joined 三种方式，两个容器要能通信，必须要有属于同一个网络的网卡
- 内嵌的 DNS server 使容器可直接通过“容器名”通信（限制：只能在 user-defined 网络中使用）
- joined 容器非常特别，可使两个或多个容器共享一个网络栈，共享网卡和配置信息，joined 容器之间可通过 127.0.0.1 直接通信。使用 --network=container:web1 指定 jointed 容器共享 web1 网络栈。使用场景：不同容器中的程序希望通过 loopback 高效快速地通信，比如 web server 与 app server。希望监控其他容器的网络流量，比如运行在独立容器中的网络监控程序。

> 容器访问外网
1. 容器 发送 ping 包至外网
2. docker0 收到包，发现是发送到外网，交给 NAT 处理
3. NAT 将源地址换成网卡外网IP
4. ping 包从网卡发送出去，到达外网

> 外网访问容器
1. 将容器对外提供服务端口映射到 host 某个端口，外网通过该端口访问容器。容器启动时通过-p参数映射端口
2. docker-proxy 监听 host 某个端口（**ps -ef | grep docker-proxy** 查看映射关系）
3. 当 curl 访问host端口时，docker-proxy 按规则转发给容器内部端口（**docker port 容器名** 查看容器映射端口）
4. 容器响应请求并返回结果

# 容器存储
> 容器存储方式
- Docker 支持多种 storage driver： AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。容器删除时存放在容器层中的工作数据也一起被删除
- 没有哪个driver能够适应所有场景，优先使用Linux发行版默认 storage driver，使用 **docker info** 可查看
- Data Volume 本质上是 Docker Host 文件系统中的目录或文件，能够直接被 mount 到容器的文件系统中，数据可以被永久的保存，即使使用它的容器已经销毁。docker 提供了两种类型的volume：bind mount 和 docker managed volume
- bind mount 是将 host 上已存在的目录或文件挂载到容器，-v 的格式为 <host path>:<container path> ，ro 设置了只读权限。
- docker managed volume 与 bind mount 在使用上最大区别是无需指定mount源，指明mount point就可以了。使用 **docker inspect 容器** 在"Mounts"中查看在host中具体目录
- 使用 **docker cp** 可以在容器和 host 之间拷贝数据

> 容器存储对比

|对比|bind moun|docker managed volume|
|-|:-:|-:|
|volume 位置|可任意指定|/var/lib/docker/volumes/...|
|对已有mount point 影响|隐藏并替换为 volume|原有数据复制到 volume|
|是否支持单个文件|支持|不支持，只能是目录|
|权限控制|可设置为只读，默认为读写权限|无控制，均为读写权限|
|移植性|移植性弱，与 host path 绑定|移植性强，无需指定 host 目录|

> 容器与host共享数据
1. 通过将共享数据放在 bind mount 中，然后将其挂载到多个容器
2. 通过 volume container 专门为容器提供 volume 的容器（使用 **docker create** 命令创建，其他容器通过 **--volumes-from** 使用）。它提供的卷可以是 bind mount，也可是 docker managed volume。volume container 优点如下：
- 与 bind mount 相比，不必为每个容器指定 host path，容器只需与 volume container 关联，实现了容器与 host 的解耦
- 使用 volume container 的容器其 mount point 是一致的，有利于配置的规范和标准化，但也带来一定的局限，使用时需要综合考虑
3. data-packed volume container 其原理是将数据打包到镜像中，然后通过 docker managed volume 共享（在Dockfile中使用VOLUME构建，其他容器通过 **--volumes-from** 使用）。它不依赖 host 提供数据，具有很强的移植性，非常适合只使用静态数据的场景，比如应用的配置信息、web server的静态文件等。

> volume 删除
- **docker rm** 删除容器时可以带上 **-v** 参数
- 如删除容器时没有带 -v ，可通过 **docker volume ls** 查看，用 **docker volume rm** 删除

# 跨主机网络
- docker 原生的 overlay 和 macvlan。第三方常用的包括 flannel、weave 和 calico等
- 第三方方案通过 libnetwork 以及 CNM 与 docker 集成

> libnetwork & CNM
- libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成
1. **Sandbox** 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。
2. **Endpoint** 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。
3. **Network** 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。

> overlay driver
- overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性
- overlay 网络需要一个 key-value 数据库用于保存网络状态信息，包括 Network、Endpoint、IP 等。Consul、Etcd 和 ZooKeeper 都是 Docker 支持的 key-vlaue 软件
- 通过修改 host 的 docker daemon 的配置文件/etc/systemd/system/docker.service（**--cluster-store** 指定 KV 的地址，**--cluster-advertise** 告知 KV 自己的连接地址），重启 docker daemon，然后创建容器网络时可以使用 **-d overlay** 指定 driver 为 overlay
- 容器启动连接 overlay后，**ip r**查看容器路由表，有两个网络接口 eth0 和 eth1，docker 还会创建一个 bridge 网络 “docker_gwbridge”，为所有连接到 overlay 网络的容器提供访问外网的能力
- docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个 tunnel 通信的
- 不同 overlay 网络之间是隔离的，即便是通过 docker_gwbridge 也不能通信，docker 默认为 overlay 网络分配 24 位掩码的子网（10.0.X.0/24），所有主机共享这个 subnet，容器启动时会顺序从此空间分配 IP。也可以通过 --subnet 指定 IP 空间

> macvlan driver
- macvlan 本身是 linxu kernel 模块，其功能是允许在同一个物理网卡上配置多个 MAC 地址，即多个 interface，每个 interface 可以配置自己的 IP，本质上是一种网卡虚拟化技术
- macvlan 的最大优点是性能极好，相比其他实现，macvlan 不需要创建 Linux bridge，而是直接通过以太 interface 连接到物理网络
- 创建网络时使用 **-d macvlan** 指定 driver 为 macvlan，macvlan 网络是 local 网络，为了保证跨主机能够通信，用户需要自己管理 IP subnet，通过 -o parent 指定使用的网络 interface
- 与其他网络不同，docker 不会为 macvlan 创建网关，这里的网关应该是真实存在的，否则容器无法路由。docker 没有为 macvlan 提供 DNS 服务，这点与 overlay 网络是不同的
- macvlan 会独占主机的网卡，也就是说一个网卡只能创建一个 macvlan 网络，通过 sub-interface 可以实现多 macvlan 网络，不同 macvlan 网络不能在二层上通信。在三层上可以通过网关将 macvlan 连通
- macvlan 网络的连通和隔离完全依赖 VLAN、IP subnet 和路由，docker 本身不做任何限制，用户可以像管理传统 VLAN 网络那样管理 macvlan

> Flannel 
- flannel 是 CoreOS 开发的容器网络解决方案。flannel 为每个 host 分配一个 subnet，容器从此 subnet 中分配 IP，这些 IP 可以在 host 间路由，容器间无需 NAT 和 port mapping 就可以跨主机通信
- 每个 subnet 都是从一个更大的 IP 池中划分的，flannel 会在每个主机上运行一个叫 flanneld 的 agent，其职责就是从池子中分配 subnet
- 为了在各个主机间共享信息，flannel 用 etcd 存放网络配置、已分配的 subnet、host 的 IP 等信息
- 数据包在主机间转发由 backend 实现。flannel 提供了多种 backend，最常用的有 vxlan 和 host-gw
- flannel 没有创建新的 docker 网络，而是直接使用默认的 bridge 网络。同一主机的容器通过 docker0 连接，跨主机流量通过 flannel.1 将数据包封装成 VxLAN，通过网卡发送给其他host
- flannel 为每个主机分配了独立的 subnet，但 flannel.1 将这些 subnet 连接起来了，相互之间可以路由。本质上，flannel 将各主机上相互独立的 docker0 容器网络组成了一个互通的大网络，实现了容器跨主机通信。flannel 没有提供隔离
- 与 vxlan 不同，host-gw 不会封装数据包，而是在主机的路由表中创建到其他主机 subnet 的路由条目，从而实现容器跨主机通信。下面对 host-gw 和 vxlan 这两种 backend 做个简单比较。
1. host-gw 把每个主机都配置成网关，主机知道其他主机的 subnet 和转发地址。vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。
2. 虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变。由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。

> Weave 
- weave 是 Weaveworks 开发的容器网络解决方案。weave 创建的虚拟网络可以将部署在多个主机上的容器连接起来。对容器来说，weave 就像一个巨大的以太网交换机，所有容器都被接入这个交换机，容器可以直接通信，无需 NAT 和端口映射。除此之外，weave 的 DNS 模块使容器可以通过 hostname 访问
- weave 不依赖分布式数据库交换网络信息，每个主机上只需运行 weave 组件就能建立起跨主机容器网络。weave 会创建一个新的 Docker 网络 weave，在host上运行了三个容器：
1. weave 是主程序，负责建立 weave 网络、收发数据 、提供 DNS 服务等。
2. weaveplugin 是 libnetwork CNM driver，实现 Docker 网络。
3. weaveproxy 提供 Docker 命令的代理服务，当用户运行 Docker CLI 创建容器时，它会自动将容器添加到 weave 网络。

- weave 网络包含两个虚拟交换机：Linux bridge weave 和 Open vSwitch datapath，veth pair vethwe-bridge 和 vethwe-datapath 将二者连接在一起。weave 和 datapath 分工不同，weave 负责将容器接入 weave 网络，datapath 负责在主机间 VxLAN 隧道中并收发数据
- 默认配置下，weave 使用一个大 subnet（例如 10.32.0.0/12），所有主机的容器都从这个地址空间中分配 IP，因为同属一个 subnet，容器可以直接通信。如果要实现网络隔离，可以通过环境变量 WEAVE_CIDR 为容器分配不同 subnet 的 IP
- weave 是一个私有的 VxLAN 网络，默认与外部网络隔离。执行 weave expose 可将主机加入到 weave，然后把主机当作访问 weave 网络的网关。weave 网桥位于 root namespace，它负责将容器接入 weave 网络。给 weave 配置同一 subnet 的 IP 其本质就是将 host 接入 weave 网络
- 10.32.0.0/12 是 weave 网络使用的默认 subnet，如果此地址空间与现有 IP 冲突，可以通过 --ipalloc-range 分配特定的 subnet，不过请确保所有 host 都使用相同的 subnet

> Calico 
- Calico 是一个纯三层的虚拟网络方案，Calico 为每个容器分配一个 IP，每个 host 都是 router，把不同 host 的容器连接起来。与 VxLAN 不同的是，Calico 不对数据包做额外封装，不需要 NAT 和端口映射，扩展性和性能都很好
- 与其他容器网络方案相比，Calico 还有一大优势：network policy。用户可以动态定义 ACL 规则，控制进出容器的数据包，精细化控制进出容器的流量
- Calico 依赖 etcd 在不同主机间共享和交换信息，存储 Calico 网络状态。Calico 网络中的每个主机都需要运行 Calico 组件，提供容器 interface 管理、动态路由、动态 ACL、报告状态等功能。**--driver calico** 指定使用 calico 的 libnetwork CNM driver。**--ipam-driver calico-ipam** 指定使用 calico 的 IPAM driver 管理 IP
- calico 默认的 policy 规则是：容器只能与同一个 calico 网络中的容器通信。calico 的每个网络都有一个同名的 profile，profile 中定义了该网络的 policy

# 主流跨主机容器网络方案对比
> 网络模型
- Docker overlay 如名称所示，是 overlay 网络，建立主机间 VxLAN 隧道，原始数据包在发送端被封装成 VxLAN 数据包，到达目的后在接收端解包
- Macvlan 网络在二层上通过 VLAN 连接容器，在三层上依赖外部网关连接不同 macvlan。数据包直接发送，不需要封装，属于 underlay 网络
- Flannel 我们讨论了两种 backend：vxlan 和 host-gw。vxlan 与 Docker overlay 类似，属于 overlay 网络。host-gw 将主机作为网关，依赖三层 IP 转发，不需要像 vxlan 那样对包进行封装，属于 underlay 网络。
- Weave 是 VxLAN 实现，属于 overlay 网络
- Calico 和 host-gw 一样依赖三层 IP 转发，属于 underlay 网络

> Distributed Store 存储网络信息
- Docker Overlay、Flannel 和 Calico 都需要 etcd 或 consul
- Macvlan 是简单的 local 网络，不需要保存和共享网络信息
- Weave 自己负责在主机间交换网络配置信息，也不需要 Distributed Store

> IPAM 容器网络的 IP 管理
- Docker Overlay 网络中所有主机共享同一个 subnet，容器启动时会顺序分配 IP，可以通过 --subnet 定制 IP 空间
- Macvlan 需要用户自己管理 subnet，为容器分配 IP，不同 subnet 通信依赖外部网关
- Flannel 为每个主机自动分配独立的 subnet，用户只需要指定一个大的 IP 池。不同 subnet 之间的路由信息也由 Flannel 自动生成和配置
- Weave 默认配置下所有容器使用 10.32.0.0/12 subnet，如果此地址空间与现有 IP 冲突，可以通过 --ipalloc-range 分配特定的 subnet
- Calico 从 IP Pool（可定制）中为每个主机分配自己的 subnet

> 连通与隔离
- 同一 Docker Overlay 网络中的容器可以通信，但不同网络之间无法通信，要实现跨网络访问，只有将容器加入多个网络。与外网通信可通过 docker_gwbridge 网络
- Macvlan 网络的连通或隔离完全取决于二层 VLAN 和三层路由
- 不同 Flannel 网络中的容器直接就可以通信，没有提供隔离。与外网通信可以通过 bridge 网络
- Weave 网络默认配置下所有容器在一个大的 subnet 中，可以自由通信，如果要实现隔离，需要为容器指定不同的 subnet 或 IP。与外网通信的方案是将主机加入到 weave 网络，并把主机当作网关。
- Calico 默认配置下只允许位于同一网络中的容器之间通信，但通过其强大的 Policy 能够实现几乎任意场景的访问控制

> 性能(理论)
- 理论上 Underlay 网络性能优于 Overlay 网络。Overlay 网络利用隧道技术，将数据包封装到 UDP 中进行传输。因涉及数据包的封装和解封，存在额外的 CPU 和网络开销。虽然几乎所有 Overlay 网络方案底层都采用 Linux kernel 的 vxlan 模块，这样可以尽量减少开销，但这个开销与 Underlay 网络相比还是存在的
- 所以 Macvlan、Flannel host-gw、Calico 的性能会优于 Docker overlay、Flannel vxlan 和 Weave
- Overlay 较 Underlay 可以支持更多的二层网段，能更好地利用已有网络，以及有避免物理交换机 MAC 表耗尽等优势

# 跨主机存储
- docker 通过 volume driver 实现跨主机管理 data volume。创建 volume 时如不特别指定，将使用 local 类型的 driver，即从 Docker Host 的本地目录中分配存储空间。如要支持跨主机的 volume，则需要使用第三方 driver。
- 目前已有很多可用的 driver，如 Azure File Storage、GlusterFS、REX-Ray、VMware vSphere Storage 等，完整列表可参考 https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins
- 在 docker 上执行**docker volume create --driver xxx --name=xxxdata --opt=size=2** 来命令创建 volume，然后使用 **-v xxxdata:/var/lib/data** 将之前创建的 volume mount 到 容器目录
